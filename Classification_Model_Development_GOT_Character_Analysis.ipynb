{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part I: Preparation and Exploration</h2>\n",
    "<br><h3>a) Imports and Loading the Dataset</h3><br>\n",
    "    Import packages and load the 'GOT_character_predictions.xlsx' dataset into Python as <strong>GOT</strong> (from the <em>Downloads</em> folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing gender_guesser\n",
    "# %pip install gender_guesser                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas            as pd                       # data science essentials\n",
    "import matplotlib.pyplot as plt                      # data visualization\n",
    "import seaborn           as sns                      # enhanced data viz\n",
    "from sklearn.model_selection import train_test_split # train-test split\n",
    "from sklearn.linear_model import LogisticRegression  # logistic regression\n",
    "import statsmodels.formula.api as smf                # logistic regression\n",
    "from sklearn.metrics import confusion_matrix         # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score            # auc score\n",
    "from sklearn.neighbors import KNeighborsClassifier   # KNN for classification\n",
    "from sklearn.neighbors import KNeighborsRegressor    # KNN for regression\n",
    "from sklearn.preprocessing import StandardScaler     # standard scaler\n",
    "from sklearn.tree import DecisionTreeClassifier      # classification trees\n",
    "from sklearn.tree import plot_tree                   # tree plots                         \n",
    "import gender_guesser.detector as gender             # guess gender based on (given) name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "GOT = pd.read_excel('./GOT_character_predictions.xlsx')\n",
    "\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "# displaying the head of the dataset\n",
    "GOT.head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-defined functions\n",
    "\n",
    "#########################\n",
    "# mv_flagger\n",
    "#########################\n",
    "def mv_flagger(df):\n",
    "    \"\"\"\n",
    "Flags all columns that have missing values with 'm-COLUMN_NAME'.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "df : DataFrame to flag missing values\n",
    "\n",
    "\n",
    "RETURNS\n",
    "-------\n",
    "DataFrame with missing value flags.\"\"\"\n",
    "\n",
    "\n",
    "    for col in df:\n",
    "\n",
    "        if df[col].isnull().astype(int).sum() > 0:\n",
    "            df['m_'+col] = df[col].isnull().astype(int)\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "# text_split_feature\n",
    "#########################\n",
    "def text_split_feature(col, df, sep=' ', new_col_name='number_of_names'):\n",
    "    \"\"\"\n",
    "Splits values in a string Series (as part of a DataFrame) and sums the number\n",
    "of resulting items. Automatically appends summed column to original DataFrame.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "col          : column to split\n",
    "df           : DataFrame where column is located\n",
    "sep          : string sequence to split by, default ' '\n",
    "new_col_name : name of new column after summing split, default\n",
    "               'number_of_names'\n",
    "\"\"\"\n",
    "    \n",
    "    df[new_col_name] = 0\n",
    "    \n",
    "    \n",
    "    for index, val in df.iterrows():\n",
    "        df.loc[index, new_col_name] = len(df.loc[index, col].split(sep = ' '))\n",
    "        \n",
    "\n",
    "########################################\n",
    "# optimal_neighbors\n",
    "########################################\n",
    "def optimal_neighbors(x_data,\n",
    "                      y_data,\n",
    "                      standardize = True,\n",
    "                      pct_test=0.10,\n",
    "                      seed=219,\n",
    "                      response_type='reg',\n",
    "                      max_neighbors=20,\n",
    "                      show_viz=True):    \n",
    "    \"\"\"\n",
    "Exhaustively compute training and testing results for KNN across\n",
    "[1, max_neighbors]. Outputs the maximum test score and (by default) a\n",
    "visualization of the results.\n",
    "PARAMETERS\n",
    "----------\n",
    "x_data        : explanatory variable data\n",
    "y_data        : response variable\n",
    "standardize   : whether or not to standardize the x data, default True\n",
    "pct_test      : test size for training and validation from (0,1), default 0.25\n",
    "seed          : random seed to be used in algorithm, default 219\n",
    "response_type : type of neighbors algorithm to use, default 'reg'\n",
    "    Use 'reg' for regression (KNeighborsRegressor)\n",
    "    Use 'class' for classification (KNeighborsClassifier)\n",
    "max_neighbors : maximum number of neighbors in exhaustive search, default 20\n",
    "show_viz      : display or surpress k-neigbors visualization, default True\n",
    "\"\"\"      \n",
    "    if standardize == True:\n",
    "        # optionally standardizing x_data\n",
    "        scaler             = StandardScaler()\n",
    "        scaler.fit(x_data)\n",
    "        x_scaled           = scaler.transform(x_data)\n",
    "        x_scaled_df        = pd.DataFrame(x_scaled)\n",
    "        x_data             = x_scaled_df\n",
    "\n",
    "\n",
    "\n",
    "    # train-test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data,\n",
    "                                                        y_data,\n",
    "                                                        test_size = pct_test,\n",
    "                                                        random_state = seed)\n",
    "\n",
    "\n",
    "    # creating lists for training set accuracy and test set accuracy\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    \n",
    "    # setting neighbor range\n",
    "    neighbors_settings = range(1, max_neighbors + 1)\n",
    "\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # building the model based on response variable type\n",
    "        if response_type == 'reg':\n",
    "            clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "            clf.fit(x_train, y_train)\n",
    "            \n",
    "        elif response_type == 'class':\n",
    "            clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "            clf.fit(x_train, y_train)            \n",
    "            \n",
    "        else:\n",
    "            print(\"Error: response_type must be 'reg' or 'class'\")\n",
    "        \n",
    "        \n",
    "        # recording the training set accuracy\n",
    "        training_accuracy.append(clf.score(x_train, y_train))\n",
    "    \n",
    "        # recording the generalization accuracy\n",
    "        test_accuracy.append(clf.score(x_test, y_test))\n",
    "\n",
    "\n",
    "    # optionally displaying visualization\n",
    "    if show_viz == True:\n",
    "        # plotting the visualization\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "        plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "        plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(\"n_neighbors\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # returning optimal number of neighbors\n",
    "    print(f\"The optimal number of neighbors is: {test_accuracy.index(max(test_accuracy))+1}\")\n",
    "    return test_accuracy.index(max(test_accuracy))+1\n",
    "\n",
    "\n",
    "########################################\n",
    "# visual_cm\n",
    "########################################\n",
    "def visual_cm(true_y, pred_y, labels = None):\n",
    "    \"\"\"\n",
    "Creates a visualization of a confusion matrix.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "true_y : true values for the response variable\n",
    "pred_y : predicted values for the response variable\n",
    "labels : , default None\n",
    "    \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Loading GOT Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling up data dictionary\n",
    "GOT_description = pd.read_excel('./GOT_data_dictionary.xlsx')\n",
    "    \n",
    "\n",
    "# displaying the data dictionary\n",
    "GOT_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II - Dataset Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Missing value detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null value count\n",
    "GOT.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFOrmation about each variable\n",
    "GOT.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Flagging missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the mv_flagger function\n",
    "GOT = mv_flagger(df = GOT)\n",
    "\n",
    "# checking results\n",
    "GOT.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  c. Develop missing value and categorical encoding strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name              - (discrete) feature out, drop original    \n",
    "# title             - (categorical) impute with 'Unknown', then one-hot encode, drop original\n",
    "# gender_guess      - (categorical) impute with 'Unknown', then one-hot encode, drop original\n",
    "# culture           - (categorical) impute with 'Unknown', then one-hot encode, drop original\n",
    "# dateOfBirth       - drop as it is correlated with age\n",
    "# mother            - (categorical) drop (very dirty; insufficient domain knowledge)\n",
    "# father            - (categorical) drop (very dirty; insufficient domain knowledge)\n",
    "# heir              - (categorical) drop (very dirty; insufficient domain knowledge)\n",
    "# house             - (discrete) feature out, drop original  \n",
    "# spouse            - (categorical) drop (very dirty; insufficient domain knowledge)\n",
    "# isAliveMother     - (categorical) drop (very dirty; insufficient domain knowledge)\n",
    "# isAliveFather     - (categorical) drop (very dirty; insufficient domain knowledge)\n",
    "# isAliveHeir       - (categorical) drop (very dirty; insufficient domain knowledge)\n",
    "# isAliveSpouse     - (categorical) drop (very dirty; insufficient domain knowledge)\n",
    "# isMarried         - (categorical) keep original\n",
    "# isNoble           - (categorical) keep original\n",
    "# age               - (continuous) impute median as it has negative value and also using gender_guess field\n",
    "# numDeadRelations  - (continuous) keep original\n",
    "# popularity        - (continuous) keep original\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Gender guesser\n",
    "\n",
    "#### Below is the actual gender guess code. Commented as it consumes time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # guessing gender based on (given) name\n",
    "\n",
    "# # placeholder list\n",
    "# placeholder_lst = []\n",
    "\n",
    "\n",
    "# # looping to guess gender\n",
    "# for name in GOT['name']:\n",
    "#     name.split()\n",
    "#     first_name = name.split()[0]\n",
    "# #    print(first_name)\n",
    "#     guess = gender.Detector().get_gender(first_name)\n",
    "# #    print(guess)\n",
    "#     placeholder_lst.append(guess)\n",
    "\n",
    "# # converting list into a series\n",
    "# GOT['gender_guess'] = pd.Series(placeholder_lst)\n",
    "\n",
    "\n",
    "# # checking results\n",
    "# GOT.head(n = 5)\n",
    "\n",
    "# # checking results\n",
    "# GOT['gender_guess'].value_counts(normalize = False,\n",
    "#                                  sort      = True,\n",
    "#                                  ascending = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the hardcoded gender guess code. This helps save time. \n",
    "##### This field is used to fill the Null values for age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guessing gender based on (given) name\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "\n",
    "# looping to guess gender\n",
    "for name in GOT['name']:\n",
    "    name.split()\n",
    "    first_name = name.split()[0]\n",
    "    \n",
    "    if first_name in ['Addison',\t'Red',\t'Rusty',\t'Tal',\t'Aubrey',\t'Dorren',\t'Moon']:\n",
    "        guess = 'andy'\n",
    "        placeholder_lst.append(guess)\n",
    "    \n",
    "    elif first_name in ['Will',\t'Robin',\t'Rowan',\t'Sam',\t'Courtenay',\t'Dale',\t'Kirby',\t'Long',\t'Ellery',\t'Bryce',\t'Devan']:\n",
    "        guess = 'mostly_male'\n",
    "        placeholder_lst.append(guess)        \n",
    "        \n",
    "    elif first_name in ['Young',\t'Sky',\t'Alyn',\t'Cass',\t'Kyle',\t'Nan',\t'Gilly',\t'Lyn',\t'Val']:\n",
    "        guess = 'mostly_female'\n",
    "        placeholder_lst.append(guess)            \n",
    "\n",
    "    elif first_name in ['Sylva',\t'Willow',\t'Wenda',\t'Randa',\t'Rhonda',\t'Rosamund',\t'Rus',\t'Serra',\t'Shella',\t'Talea',\t'Violet',\t'Walda',\t'Alia',\t'Alyce',\t'Alys',\t'Alyssa',\t'Amabel',\t'Barbara',\t'Becca',\t'Bella',\t'Bess',\t'Bethany',\t'Dalla',\t'Darla',\t'Del',\t'Denyse',\t'Donella',\t'Dorcas',\t'Eden',\t'Eglantine',\t'Elza',\t'Emma',\t'Euron',\t'Fern',\t'Ferny',\t'Hali',\t'Helly',\t'Jayde',\t'Jayne',\t'Jocelyn',\t'Joanna',\t'Johanna',\t'Kym',\t'Leana',\t'Leona',\t'Leyla',\t'Lia',\t'Liane',\t'Lysa',\t'Maddy',\t'Maggy',\t'Marei',\t'Maris',\t'Myrtle',\t'Nella',\t'Penny',\t'Alla',\t'Anya',\t'Beren',\t'Beth',\t'Corliss',\t'Deana',\t'Della',\t'Eleanor',\t'Elinor',\t'Janna',\t'Joy',\t'Kyra',\t'Leonella',\t'Lyra',\t'Margot',\t'Marianne',\t'Mariya',\t'Marissa',\t'Meredyth',\t'Mina',\t'Munda',\t'Myrielle',\t'Rhea',\t'Aurane',\t'Danelle',\t'Marya',\t'Masha',\t'Meera',\t'Melissa',\t'Mya',\t'Pia',\t'Aeron',\t'Asha',\t'Barba',\t'Elia',\t'Genna',\t'Harma',\t'Holly',\t'Myranda',\t'Shireen',\t'Robyn',\t'Arianne',\t'Meg',]:\n",
    "        guess = 'female'\n",
    "        placeholder_lst.append(guess) \n",
    "\n",
    "    elif first_name in ['Wilbert',\t'Willem',\t'Willis',\t'Joffrey',\t'Quentin',\t'Raymond',\t'Raymund',\t'Richard',\t'Rob',\t'Rickard',\t'Robb',\t'Robert',\t'Roger',\t'Roland',\t'Rolland',\t'Ronald',\t'Rupert',\t'Simon',\t'Stafford',\t'Stone',\t'Symon',\t'Terrance',\t'Theo',\t'Theobald',\t'Tim',\t'Timon',\t'Tom',\t'Tristan',\t'Ulf',\t'Umar',\t'Victor',\t'Walton',\t'Wendell',\t'Alaric',\t'Andrey',\t'Armen',\t'Arron',\t'Ben',\t'Benedict',\t'Bennet',\t'Bertram',\t'Blane',\t'Boy',\t'Brandon',\t'Bryan',\t'Buford',\t'Byron',\t'Cedric',\t'Clarence',\t'Clement',\t'Clifford',\t'Conn',\t'Damon',\t'Dan',\t'Dennis',\t'Denys',\t'Dick',\t'Dudley',\t'Duncan',\t'Edmund',\t'Egon',\t'Eldred',\t'Elyas',\t'Erik',\t'Gage',\t'Galt',\t'Gareth',\t'Garrett',\t'Garth',\t'Gavin',\t'Gerold',\t'Glendon',\t'Griffin',\t'Harbert',\t'Hamish',\t'Harlan',\t'Harry',\t'Henk',\t'Herbert',\t'Hendry',\t'Hilmar',\t'Hod',\t'Holger',\t'Hugh',\t'Jaime',\t'Jasper',\t'Jon',\t'Joss',\t'Josua',\t'Leo',\t'Lester',\t'Lew',\t'Lewis',\t'Lorimer',\t'Lothar',\t'Lucas',\t'Luke',\t'Lyonel',\t'Malcolm',\t'Manfred',\t'Martyn',\t'Matt',\t'Maynard',\t'Michael',\t'Mortimer',\t'Myles',\t'Nail',\t'Ned',\t'Ormond',\t'Owen',\t'Peter',\t'Poul',\t'Alesander',\t'Allard',\t'Allar',\t'Ambrose',\t'Andrew',\t'Armond',\t'Arthur',\t'Burton',\t'Colin',\t'Creighton',\t'Dermot',\t'Desmond',\t'Dirk',\t'Dolf',\t'Elbert',\t'Elmar',\t'Elwood',\t'Franklyn',\t'Garrison',\t'Gerald',\t'Gilbert',\t'Harlen',\t'Hugo',\t'Igon',\t'Jarl',\t'Jason',\t'Lambert',\t'Lyman',\t'Maron',\t'Mathis',\t'Morton',\t'Norbert',\t'Omer',\t'Orland',\t'Osmund',\t'Otho',\t'Philip',\t'Quincy',\t'Ralf',\t'Raynard',\t'Raynald',\t'Royce',\t'Rufus',\t'Selwyn',\t'Terrence',\t'Theodore',\t'Titus',\t'Triston',\t'Wallace',\t'Wendel',\t'William',\t'Zachery',\t'Zia',\t'Anders',\t'Arnolf',\t'Clayton',\t'Cleon',\t'Eldon',\t'Ethan',\t'Harmen',\t'Iggo',\t'Jared',\t'Jory',\t'Lyle',\t'Nestor',\t'Otto',\t'Preston',\t'Ramsay',\t'Andrik',\t'Amory',\t'Archibald',\t'Arlan',\t'Aron',\t'Bowen',\t'Cletus',\t'Danny',\t'Donal',\t'Gregor',\t'Justin',\t'Kevan',\t'Leyton',\t'Mark',\t'Marlon',\t'Wyman',\t'Rolph',\t'Damion',\t'Roslin',\t'Tycho',\t'Barra',\t'Temmo',\t'Symeon',]:\n",
    "        guess = 'male'\n",
    "        placeholder_lst.append(guess)            \n",
    "\n",
    "    else:     \n",
    "        guess = 'Unknown'\n",
    "        placeholder_lst.append(guess)         \n",
    "    \n",
    "# converting list into a series\n",
    "GOT['gender_guess'] = pd.Series(placeholder_lst)\n",
    "\n",
    "\n",
    "# checking results\n",
    "GOT.head(n = 5)\n",
    "\n",
    "# checking results\n",
    "GOT['gender_guess'].value_counts(normalize = False,\n",
    "                                 sort      = True,\n",
    "                                 ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping relative values as male and female\n",
    "GOT[\"gender_guess\"] = GOT[\"gender_guess\"].replace(to_replace = ['male','mostly_male','andy'], value = \"male\")\n",
    "GOT[\"gender_guess\"] = GOT[\"gender_guess\"].replace(to_replace = ['female','mostly_female'], value = \"female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking results\n",
    "GOT['gender_guess'].value_counts(normalize = False,\n",
    "                                 sort      = True,\n",
    "                                 ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummies for gender guess field\n",
    "dummies_gender_guess = pd.get_dummies(GOT.gender_guess, prefix='gg')\n",
    "\n",
    "dummies_gender_guess.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the appropriate values to the dataframe dropping atleast one dummy variable\n",
    "GOT = pd.concat([GOT, dummies_gender_guess[['gg_Unknown', 'gg_male']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Age Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing age field with median as it has negative values\n",
    "print(f\"\"\"\n",
    "Median Age:                     {round(GOT.loc[ : , 'age'].median(),1)}\n",
    "Median Male Age:                {round(GOT.loc[ : , 'age'][GOT['gender_guess'] == \"male\"].median(),1)}\n",
    "Median Female Age:              {round(GOT.loc[ : , 'age'][GOT['gender_guess'] == \"female\"].median(),1)}\n",
    " \"\"\")\n",
    "\n",
    "Median_Age = round(GOT.loc[ : , 'age'].median(),1)\n",
    "Median_Male_Age = round(GOT.loc[ : , 'age'][GOT['gender_guess'] == \"male\"].median(),1)\n",
    "Median_Female_Age = round(GOT.loc[ : , 'age'][GOT['gender_guess'] == \"female\"].median(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-imputation check\n",
    "GOT['age'].head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing missing values for age\n",
    "for index, val in GOT.iterrows():\n",
    "\n",
    "    \n",
    "    # imputing age for females\n",
    "    if (str(GOT.loc[index, 'age']).lower()  == 'nan' and GOT.loc[index, 'gender_guess']  == \"female\"):       \n",
    "            GOT.loc[index, 'age'] = Median_Female_Age   \n",
    "\n",
    "    # imputing age for males\n",
    "    elif (str(GOT.loc[index, 'age']).lower()  == 'nan' and GOT.loc[index, 'gender_guess']  == \"male\"):       \n",
    "              GOT.loc[index, 'age'] = Median_Male_Age \n",
    "\n",
    "    # imputing age for males\n",
    "    elif (str(GOT.loc[index, 'age']).lower()  == 'nan' and GOT.loc[index, 'gender_guess']  == \"Unknown\"):       \n",
    "              GOT.loc[index, 'age'] = Median_Age \n",
    "            \n",
    "# ensuring all missing values for age are taken care of\n",
    "print(f\"Remaining missing values for age: {GOT.loc[ :, 'age'].isnull().sum()}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-imputation check\n",
    "GOT['age'].head(n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOT features\n",
    "GOT.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram to identify skewness in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of dead relations\n",
    "mean = GOT['numDeadRelations'].mean()\n",
    "median = GOT['numDeadRelations'].median()\n",
    "mode = GOT['numDeadRelations'].mode()[0]\n",
    "\n",
    "plt.axvline(mean, color='r', linestyle='--')\n",
    "plt.axvline(median, color='g', linestyle='-')\n",
    "plt.axvline(mode, color='b', linestyle='-')\n",
    "plt.legend({'Mean':mean,'Median':median,'Mode':mode})\n",
    "\n",
    "sns.histplot(data  = GOT,\n",
    "             x     = 'numDeadRelations',\n",
    "            kde    = True)\n",
    "\n",
    "\n",
    "# title and axis labels\n",
    "plt.title(label   = \"Original Distribution of number of Dead Relations\")\n",
    "plt.xlabel(xlabel = \"number of Dead Relations\") # avoiding using dataset labels\n",
    "plt.ylabel(ylabel = \"count\")\n",
    "\n",
    "# displaying the histogram\n",
    "plt.show()\n",
    "\n",
    "#_____________________________________________________________________________\n",
    "\n",
    "# Popularity\n",
    "mean = GOT['popularity'].mean()\n",
    "median = GOT['popularity'].median()\n",
    "mode = GOT['popularity'].mode()[0]\n",
    "\n",
    "plt.axvline(mean, color='r', linestyle='--')\n",
    "plt.axvline(median, color='g', linestyle='-')\n",
    "plt.axvline(mode, color='b', linestyle='-')\n",
    "plt.legend({'Mean':mean,'Median':median,'Mode':mode})\n",
    "\n",
    "sns.histplot(data  = GOT,\n",
    "             x     = 'popularity',\n",
    "            kde    = True)\n",
    "\n",
    "\n",
    "# title and axis labels\n",
    "plt.title(label   = \"Original Distribution of popularity\")\n",
    "plt.xlabel(xlabel = \"popularity\") # avoiding using dataset labels\n",
    "plt.ylabel(ylabel = \"count\")\n",
    "\n",
    "# displaying the histogram\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying log to skewed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['log_numDeadRelations'] = np.log(GOT['numDeadRelations']+ 0.001)\n",
    "\n",
    "GOT['log_popularity'] = np.log(GOT['popularity']+ 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Culture feature cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Culture feature cleaning\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = 'Andal', value = \"Andals\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = \"Asshai'i\", value = \"Asshai\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = 'Astapori', value = \"Astapor\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = 'Braavosi', value = \"Braavos\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = ['Dornishmen', 'Dorne'], value = \"Dornish\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = ['Wildling', 'First men', 'Free Folk', 'free folk', 'Free folk'], value = \"Wildlings\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = ['Ghiscaricari',  'Ghis'], value = \"Ghiscari\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = 'Ironmen', value = \"Ironborn\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = 'Lhazarene', value = \"Lhazareen\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = 'Lyseni', value = \"Lysene\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = 'Meereenese', value = \"Meereen\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = ['Myr','Myrmen'], value = \"Myrish\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = 'Northern mountain clans', value = \"Northmen\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = 'Norvoshi', value = \"Norvos\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = 'Qartheen', value = \"Qarth\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = ['The Reach', 'Reachmen'], value = \"Reach\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = ['Rivermen'], value = \"Riverlands\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = 'Stormlands', value = \"Stormlander\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = ['Summer Islander', 'Summer Isles'], value = \"Summer Islands\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = ['Valemen', 'Vale mountain clans'], value = \"Vale\")\n",
    "GOT[\"culture\"] = GOT[\"culture\"].replace(to_replace = ['Westerman', 'Westerlands'], value = \"Westermen\")\n",
    "\n",
    "# Fill NaN with 'Unknown'\n",
    "GOT['culture'] = GOT['culture'].fillna('Unknown')\n",
    "\n",
    "GOT['culture'].value_counts(normalize = False,\n",
    "                          sort      = True,\n",
    "                          ascending = False)\n",
    "\n",
    "# create dummies for culture variable\n",
    "dummies_culture = pd.get_dummies(GOT.culture, prefix='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the big culture groups to the dataframe\n",
    "GOT = pd.concat([GOT, dummies_culture[['c_Unknown', 'c_Northmen', 'c_Ironborn','c_Wildlings','c_Valyrian','c_Braavos','c_Dornish']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g. Developing number of names features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling text_split_feature\n",
    "text_split_feature(col = 'name',\n",
    "                   df  = GOT)\n",
    "\n",
    "\n",
    "# checking results\n",
    "GOT['number_of_names'].value_counts(normalize = False,\n",
    "                                        sort      = False,\n",
    "                                        ascending = False).sort_index()\n",
    "\n",
    "GOT.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h. Title feature imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN with 'Unknown'\n",
    "GOT['title'] = GOT['title'].fillna('Unknown')\n",
    "\n",
    "GOT['title'].value_counts(normalize = False,\n",
    "                          sort      = True,\n",
    "                          ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate dummies for the title variable\n",
    "dummies_title = pd.get_dummies(GOT.title, prefix='t')\n",
    "\n",
    "dummies_title.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the titles with highest numbers to the dataframe\n",
    "GOT = pd.concat([GOT, dummies_title[['t_Unknown', 't_Ser','t_Maester','t_Lord','t_Archmaester','t_Septon']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. House feature imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN with 'Unknown'\n",
    "GOT['house'] = GOT['house'].fillna('Unknown')\n",
    "\n",
    "GOT['house'].value_counts(normalize = False,\n",
    "                          sort      = True,\n",
    "                          ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate dummies for the house variable\n",
    "dummies_house = pd.get_dummies(GOT.house, prefix='h')\n",
    "\n",
    "dummies_house.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the houses with highest numbers to the dataframe\n",
    "GOT = pd.concat([GOT, dummies_house[['h_Unknown', \"h_Night's Watch\",'h_House Frey','h_House Stark','h_House Targaryen']]], axis=1)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### j. Missing value verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null value count\n",
    "GOT.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k. Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_age_popularity'] = GOT['age'] * GOT['popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book1_popularity'] = GOT['book1_A_Game_Of_Thrones'] * GOT['log_popularity']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book2_popularity'] = GOT['book2_A_Clash_Of_Kings'] * GOT['log_popularity']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book3_popularity'] = GOT['book3_A_Storm_Of_Swords'] * GOT['log_popularity']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book4_popularity'] = GOT['book4_A_Feast_For_Crows'] * GOT['log_popularity']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book5_popularity'] = GOT['book5_A_Dance_with_Dragons'] * GOT['log_popularity']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book1_log_numDeadRel'] = GOT['book1_A_Game_Of_Thrones'] * GOT['log_numDeadRelations']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book2_log_numDeadRel'] = GOT['book2_A_Clash_Of_Kings'] * GOT['log_numDeadRelations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book3_log_numDeadRel'] = GOT['book3_A_Storm_Of_Swords'] * GOT['log_numDeadRelations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book4_log_numDeadRel'] = GOT['book4_A_Feast_For_Crows'] * GOT['log_numDeadRelations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book5_log_numDeadRel'] = GOT['book5_A_Dance_with_Dragons'] * GOT['log_numDeadRelations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_male_log_numDeadRel'] = GOT['gg_male'] * GOT['log_numDeadRelations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_c_Northmen_popularity'] = GOT['c_Northmen'] * GOT['log_popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book1_popularity_ddrela'] = GOT['interaction_book1_popularity'] * GOT['log_numDeadRelations']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book2_popularity_ddrela'] = GOT['interaction_book2_popularity'] * GOT['log_numDeadRelations'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book3_popularity_ddrela'] = GOT['interaction_book3_popularity'] * GOT['log_numDeadRelations'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book4_popularity_ddrela'] = GOT['interaction_book4_popularity'] * GOT['log_numDeadRelations'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book5_popularity_ddrela'] = GOT['interaction_book5_popularity'] * GOT['log_numDeadRelations'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book1_popularity_married'] = GOT['interaction_book1_popularity'] * GOT['isMarried']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book2_popularity_married'] = GOT['interaction_book2_popularity'] * GOT['isMarried'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book3_popularity_married'] = GOT['interaction_book3_popularity'] * GOT['isMarried'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book4_popularity_married'] = GOT['interaction_book4_popularity'] * GOT['isMarried'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book5_popularity_married'] = GOT['interaction_book5_popularity'] * GOT['isMarried'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book1_popularity_male'] = GOT['interaction_book1_popularity'] * GOT['gg_male']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book2_popularity_male'] = GOT['interaction_book2_popularity'] * GOT['gg_male']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book3_popularity_male'] = GOT['interaction_book3_popularity'] * GOT['gg_male']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book4_popularity_male'] = GOT['interaction_book4_popularity'] * GOT['gg_male']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT['interaction_book5_popularity_male'] = GOT['interaction_book5_popularity'] * GOT['gg_male']   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l. Final columns after imputation and one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### m. Dropping addional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping categorical variables after they've been encoded\n",
    "GOT = GOT.drop(['name', 'title', 'culture',\n",
    "                        'house', 'gender_guess','popularity','numDeadRelations'], axis = 1)\n",
    "\n",
    "# further dropping categorical variables with insufficient data(domain knowledge)\n",
    "GOT = GOT.drop(['S.No', 'mother', 'father', 'spouse',\n",
    "                        'heir', 'isAliveMother','isAliveFather','isAliveHeir','isAliveSpouse'], axis = 1)\n",
    "\n",
    "# further dropping flagged values that do not have missing data \n",
    "GOT = GOT.drop(['m_title', 'm_culture', 'm_house', 'm_age'], axis = 1)\n",
    "\n",
    "# checking the results\n",
    "GOT.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part III - Logistic Regression</h2><br>\n",
    " \n",
    "### a. Correlation Analysis\n",
    "Correlations between the response variable and the explanatory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = GOT.corr(method = 'pearson').round(decimals = 2)\n",
    "\n",
    "df_corr['isAlive'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original balance between those who are alive and those who are not alive in Game of thrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT.loc[ : ,'isAlive'].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h3>b. Preparing Explanatory and Response Data</h3>\n",
    "Declare the explanatory variables as <strong>GOT_data</strong> and the response variable (survived) as <strong>GOT_response</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring explanatory variables\n",
    "# removing dateofbirth as it is highly correlated with age\n",
    "GOT_data = GOT.drop(['isAlive', 'dateOfBirth'], axis = 1)\n",
    "\n",
    "\n",
    "# declaring response variable\n",
    "GOT_target = GOT.loc[: ,'isAlive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h3>c. Prepare train-test split for statsmodels.</h3>\n",
    "The stratify argument helps preserve the balance of the response variable on in the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split with stratification\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            GOT_data,\n",
    "            GOT_target,\n",
    "            test_size    = 0.10,\n",
    "            random_state = 219,\n",
    "            stratify     = GOT_target) # preserving balance\n",
    "\n",
    "\n",
    "# merging training data for statsmodels\n",
    "GOT_train = pd.concat([x_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "\n",
    "Response Variable Proportions (Training Set)\n",
    "--------------------------------------------\n",
    "{y_train.value_counts(normalize = True).round(decimals = 2)}\n",
    "\n",
    "\n",
    "\n",
    "Response Variable Proportions (Testing Set)\n",
    "--------------------------------------------\n",
    "{y_test.value_counts(normalize = True).round(decimals = 2)}\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>d. Build a logistic regression model in statsmodels using all of the explanatory variables.</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in GOT_data:\n",
    "    print(f\" {val} + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logistic_full = smf.logit(formula = \"\"\"  isAlive ~    \n",
    "                                         book1_A_Game_Of_Thrones + \n",
    "                                         book2_A_Clash_Of_Kings + \n",
    "                                         book3_A_Storm_Of_Swords +\n",
    "                                         book4_A_Feast_For_Crows + \n",
    "                                         book5_A_Dance_with_Dragons +\n",
    "                                         log_numDeadRelations +     \n",
    "                                         m_dateOfBirth + \n",
    "                                         m_mother + \n",
    "                                         m_father + \n",
    "                                         m_heir + \n",
    "                                         isMarried +\n",
    "                                         isNoble +\n",
    "                                         gg_Unknown +\n",
    "                                         gg_male +\n",
    "                                         age +\n",
    "                                         log_popularity + \n",
    "                                         c_Unknown + \n",
    "                                         c_Northmen + \n",
    "                                         c_Ironborn + \n",
    "                                         c_Wildlings + \n",
    "                                         c_Valyrian + \n",
    "                                         c_Braavos + \n",
    "                                         c_Dornish + \n",
    "                                         number_of_names + \n",
    "                                         interaction_book1_log_numDeadRel + \n",
    "                                         interaction_book2_log_numDeadRel + \n",
    "                                         interaction_book3_log_numDeadRel + \n",
    "                                         interaction_book4_log_numDeadRel + \n",
    "                                         interaction_book5_log_numDeadRel + \n",
    "                                         interaction_book1_popularity +\n",
    "                                         interaction_book2_popularity +\n",
    "                                         interaction_book3_popularity +\n",
    "                                         interaction_book4_popularity +\n",
    "                                         interaction_book5_popularity +\n",
    "                                         interaction_book1_popularity_ddrela +\n",
    "                                         interaction_book2_popularity_ddrela +\n",
    "                                         interaction_book3_popularity_ddrela +\n",
    "                                         interaction_book4_popularity_ddrela +\n",
    "                                         interaction_book5_popularity_ddrela +\n",
    "                                         interaction_book1_popularity_male + \n",
    "                                         interaction_book2_popularity_male +                                         \n",
    "                                         interaction_book3_popularity_male + \n",
    "                                         interaction_book4_popularity_male + \n",
    "                                         interaction_book5_popularity_male +\n",
    "                                         interaction_male_log_numDeadRel \n",
    "                                         \"\"\",\n",
    "                                         data    = GOT_train)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "results_full = logistic_full.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "results_full.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>e. Model where all features are significant based on their p-values.</h3><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logit_sig = smf.logit(formula = \"\"\"  isAlive ~    \n",
    "                                         book1_A_Game_Of_Thrones + \n",
    "                                         book3_A_Storm_Of_Swords +\n",
    "                                         log_numDeadRelations +     \n",
    "                                         log_popularity +\n",
    "                                         interaction_book3_log_numDeadRel +  \n",
    "                                         interaction_book3_popularity +\n",
    "                                         interaction_book3_popularity_ddrela \n",
    "                                         \"\"\",\n",
    "                                         data    = GOT_train)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "logit_sig = logit_sig.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "logit_sig.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logit_sig_2 = smf.logit(formula = \"\"\" isAlive ~      book1_A_Game_Of_Thrones + \n",
    "                                                     book4_A_Feast_For_Crows + \n",
    "                                                     log_numDeadRelations + \n",
    "                                                     log_popularity +\n",
    "                                                     interaction_book4_log_numDeadRel                                                  \n",
    "                                                     \"\"\",\n",
    "                                     data    = GOT_train)\n",
    "\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "logit_sig_2 = logit_sig_2.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "logit_sig_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Logistic Regression in scikit-learn\n",
    "### a. Dictionary of each candidate model's explanatory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanatory sets from last session\n",
    "\n",
    "# creating a dictionary to store candidate models\n",
    "\n",
    "candidate_dict = {\n",
    "\n",
    " # full model\n",
    " 'logit_full'   : ['book1_A_Game_Of_Thrones',\t'book2_A_Clash_Of_Kings',\t'book3_A_Storm_Of_Swords',\t\n",
    "                   'book4_A_Feast_For_Crows',\t'book5_A_Dance_with_Dragons',\t'log_numDeadRelations',\t\n",
    "                   'm_dateOfBirth',\t'm_mother',\t'm_father',\t'm_heir',\t'isMarried',\t'isNoble',\t'gg_Unknown',\n",
    "                   'gg_male',\t'age',\t'log_popularity',\t'c_Unknown',\t'c_Northmen',\t'c_Ironborn',\t\n",
    "                   'c_Wildlings',\t'c_Valyrian',\t'c_Braavos',\t'c_Dornish',\t'number_of_names',\t\n",
    "                   'interaction_book1_log_numDeadRel',\t'interaction_book2_log_numDeadRel',\t\n",
    "                   'interaction_book3_log_numDeadRel',\t'interaction_book4_log_numDeadRel',\t\n",
    "                   'interaction_book5_log_numDeadRel',\t'interaction_book1_popularity',\t\n",
    "                   'interaction_book2_popularity',\t'interaction_book3_popularity',\t'interaction_book4_popularity',\t\n",
    "                   'interaction_book5_popularity',\t'interaction_book1_popularity_ddrela',\t\n",
    "                   'interaction_book2_popularity_ddrela',\t'interaction_book3_popularity_ddrela',\t\n",
    "                   'interaction_book4_popularity_ddrela',\t'interaction_book5_popularity_ddrela',\t\n",
    "                   'interaction_book1_popularity_male',\t'interaction_book2_popularity_male',\t\n",
    "                   'interaction_book3_popularity_male',\t'interaction_book4_popularity_male',\t\n",
    "                   'interaction_book5_popularity_male',\t'interaction_male_log_numDeadRel',\n",
    "  ],\n",
    " \n",
    "\n",
    " # significant variables only (set 1)\n",
    " 'logit_sig'    : ['book1_A_Game_Of_Thrones' , 'book3_A_Storm_Of_Swords' , \n",
    "                    'log_numDeadRelations' , \n",
    "                    'log_popularity' , 'interaction_book3_log_numDeadRel' ,  'interaction_book3_popularity' , \n",
    "                   'interaction_book3_popularity_ddrela'],\n",
    "    \n",
    "    \n",
    " # significant variables only (set 2)\n",
    " 'logit_sig_2'  : ['book1_A_Game_Of_Thrones' , 'book4_A_Feast_For_Crows' , \n",
    "                    'log_numDeadRelations' , 'log_popularity' ,  'interaction_book4_log_numDeadRel' ],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>b. Dynamically printing each explanatory variable set.</h3><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing candidate variable sets\n",
    "print(f\"\"\"\n",
    "/--------------------------\\\\\n",
    "|Explanatory Variable Sets |\n",
    "\\\\--------------------------/\n",
    "\n",
    "Full Model:\n",
    "-----------\n",
    "{candidate_dict['logit_full']}\n",
    "\n",
    "\n",
    "First Significant p-value Model:\n",
    "--------------------------------\n",
    "{candidate_dict['logit_sig']}\n",
    "\n",
    "\n",
    "Second Significant p-value Model:\n",
    "---------------------------------\n",
    "{candidate_dict['logit_sig_2']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "\n",
    "<h3>b) Build a logistic regression model in scikit-learn</h3>\n",
    "Building a logistic regression model in scikit-learn using the <strong>logit_sig</strong> explanatory variables and <strong>isAlive</strong> as the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the full model\n",
    "GOT_data   =  GOT.loc[ : , candidate_dict['logit_sig_2']]\n",
    "GOT_target =  GOT.loc[ : , 'isAlive']\n",
    "\n",
    "\n",
    "# This is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            GOT_data,\n",
    "            GOT_target,\n",
    "            test_size    = 0.10,\n",
    "            random_state = 219,\n",
    "            stratify     = GOT_target)\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            C = 1,\n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(x_test)\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', logreg_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', logreg_fit.score(x_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(x_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(x_test, y_test).round(4) # accuracy\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Logreg Train-Test Gap :', abs(logreg_train_score - logreg_test_score).round(4))\n",
    "logreg_test_gap = abs(logreg_train_score - logreg_test_score).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<h3>Part V: The Confusion Matrix</h3><br>\n",
    "The confusion matrix in Python can be read as follows:<br><br>\n",
    "\n",
    "~~~\n",
    "                   |\n",
    "  True Negatives   |  False Positives\n",
    "  (correct)        |  (incorrect)\n",
    "                   |\n",
    "-------------------|------------------\n",
    "                   |\n",
    "  False Negatives  |  True Positives\n",
    "  (incorrect)      |  (correct)\n",
    "                   |\n",
    "~~~\n",
    "\n",
    "<br><br>\n",
    "In terms of our model:<br><br>\n",
    "\n",
    "~~~\n",
    "                                                 |\n",
    "  PREDICTED: IS ALIVE (isAlive=0)                |  PREDICTED: IS NOT ALIVE (isAlive=1)\n",
    "  ACTUAL:    IS ALIVE (isAlive=0)                |  ACTUAL:    IS ALIVE     (isAlive=0)\n",
    "                                                 |\n",
    "-------------------------------------------------|-----------------------------------------------\n",
    "                                                 |\n",
    "  PREDICTED: IS ALIVE     (isAlive=0)            |  PREDICTED: IS NOT ALIVE (isAlive=1)\n",
    "  ACTUAL:    IS NOT ALIVE (isAlive=1)            |  ACTUAL:    IS NOT ALIVE (isAlive=1)\n",
    "                                                 |  \n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a confusion matrix\n",
    "print(confusion_matrix(y_true = y_test,\n",
    "                       y_pred = logreg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {logreg_tn}\n",
    "False Positives: {logreg_fp}\n",
    "False Negatives: {logreg_fn}\n",
    "True Positives : {logreg_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<strong>Visualized Confusion Matrix</strong><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calling the visual_cm function\n",
    "visual_cm(true_y = y_test,\n",
    "          pred_y = logreg_pred,\n",
    "          labels = ['Is Alive', 'Is Not Alive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h3>Area Under The Curve (AUC)</h3><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# area under the roc curve (auc)\n",
    "print(roc_auc_score(y_true  = y_test,\n",
    "                    y_score = logreg_pred).round(decimals = 4))\n",
    "\n",
    "\n",
    "# saving AUC score for future use\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = logreg_pred).round(decimals = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "Run the code below to observe the model's coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "logreg_model_values = zip(GOT[candidate_dict['logit_sig_2']].columns,\n",
    "                          logreg_fit.coef_.ravel().round(decimals = 2))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "logreg_model_lst = [('intercept', logreg_fit.intercept_[0].round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in logreg_model_values:\n",
    "    logreg_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in logreg_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part VI: Classification Trees (CART Models)</h2><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load a user-defined function for CART model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = x_train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('Tree_Leaf_50_Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "### 1.a. Developing a classification full tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "full_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Full Tree Train-Test Gap :', abs(full_tree_train_score - full_tree_test_score).round(4))\n",
    "full_tree_test_gap = abs(full_tree_train_score - full_tree_test_score).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h3>1.b. Confusion matrix.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "full_tree_tn, \\\n",
    "full_tree_fp, \\\n",
    "full_tree_fn, \\\n",
    "full_tree_tp = confusion_matrix(y_true = y_test, y_pred = full_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {full_tree_tn}\n",
    "False Positives: {full_tree_fp}\n",
    "False Negatives: {full_tree_fn}\n",
    "True Positives : {full_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c. Run the following code generate a visual tree output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setting figure size\n",
    "# plt.figure(figsize=(150,50))\n",
    "\n",
    "\n",
    "# # developing a plotted tree\n",
    "# plot_tree(decision_tree = full_tree_fit, \n",
    "#           feature_names = GOT.columns,\n",
    "#           filled        = True, \n",
    "#           rounded       = True, \n",
    "#           fontsize      = 14)\n",
    "\n",
    "\n",
    "# # rendering the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a. Developing a pruned classificaion tree model.\n",
    "Classification tree with a maximum depth of 5 and a minimum number of samples per leaf of 25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "pruned_tree = DecisionTreeClassifier(max_depth = 5,\n",
    "                    min_samples_leaf = 25,\n",
    "                    random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "pruned_tree_fit = pruned_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "pruned_tree_pred = pruned_tree.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', pruned_tree_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', pruned_tree_fit.score(x_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = pruned_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "pruned_tree_train_score = pruned_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "pruned_tree_test_score  = pruned_tree_fit.score(x_test, y_test).round(4) # accuracy\n",
    "\n",
    "\n",
    "# saving auc score\n",
    "pruned_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                        y_score = pruned_tree_pred).round(4) # auc\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Pruned Train-Test Gap :', abs(pruned_tree_train_score - pruned_tree_test_score).round(4))\n",
    "pruned_tree_test_gap = abs(pruned_tree_train_score - pruned_tree_test_score).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.b. Confusion matrix.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "pruned_tree_tn, \\\n",
    "pruned_tree_fp, \\\n",
    "pruned_tree_fn, \\\n",
    "pruned_tree_tp = confusion_matrix(y_true = y_test, y_pred = pruned_tree_pred.ravel()\n",
    ").ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {pruned_tree_tn}\n",
    "False Positives: {pruned_tree_fp}\n",
    "False Negatives: {pruned_tree_fn}\n",
    "True Positives : {pruned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.c. Visual Tree output.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setting figure size\n",
    "plt.figure(figsize=(20, 10)) # adjusting to better fit the visual\n",
    "\n",
    "\n",
    "# developing a plotted tree\n",
    "plot_tree(decision_tree = pruned_tree_fit, # changing to pruned_tree_fit\n",
    "          feature_names = GOT.columns,\n",
    "          filled        = True, \n",
    "          rounded       = True, \n",
    "          fontsize      = 14)\n",
    "\n",
    "\n",
    "# rendering the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d. Feature performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting feature importance\n",
    "plot_feature_importances(pruned_tree_fit,\n",
    "                         train = x_train,\n",
    "                         export = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h3>Comparing Results</h3><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# comparing results\n",
    "print(f\"\"\"\n",
    "Model         AUC Score    Training Accuracy  Testing Accuracy TN, FP, FN, TP     Train-Test gap\n",
    "-----         ---------    --------------     ---------------  --------------     --------------\n",
    "Logistic      {logreg_auc_score}        {logreg_train_score}            {logreg_test_score}           {logreg_tn, logreg_fp, logreg_fn, logreg_tp}    {logreg_test_gap}\n",
    "Full Tree     {full_tree_auc_score}        {full_tree_train_score}            {full_tree_test_score}           {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}   {full_tree_test_gap}\n",
    "Pruned Tree   {pruned_tree_auc_score}        {pruned_tree_train_score}            {pruned_tree_test_score}           {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}    {pruned_tree_test_gap}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part VII: Classification Modeling with KNN</h2><br>\n",
    "<h3>a. Running Optimal Neighbors function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining the optimal number of neighbors\n",
    "opt_neighbors = optimal_neighbors(x_data        = GOT_data,\n",
    "                                  y_data        = GOT_target,\n",
    "                                  response_type = 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### b. Scaling the explanatory data. Building a KNN classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING StandardScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# FITTING the data\n",
    "scaler.fit(GOT_data)\n",
    "\n",
    "\n",
    "# TRANSFORMING the data\n",
    "x_scaled     = scaler.transform(GOT_data)\n",
    "\n",
    "\n",
    "# converting to a DataFrame\n",
    "x_scaled_df  = pd.DataFrame(x_scaled) \n",
    "\n",
    "\n",
    "# train-test split with the scaled data\n",
    "x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled = train_test_split(\n",
    "            x_scaled_df,\n",
    "            GOT_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.10,\n",
    "            stratify     = GOT_target)\n",
    "\n",
    "\n",
    "# INSTANTIATING a KNN classification model with optimal neighbors\n",
    "knn_opt = KNeighborsClassifier(n_neighbors = opt_neighbors)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "knn_fit = knn_opt.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "knn_pred = knn_fit.predict(x_test_scaled)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', knn_fit.score(x_train_scaled, y_train_scaled).round(4))\n",
    "print('Testing  ACCURACY:', knn_fit.score(x_test_scaled, y_test_scaled).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = knn_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data\n",
    "knn_train_score = knn_fit.score(x_train_scaled, y_train_scaled).round(4)\n",
    "knn_test_score  = knn_fit.score(x_test_scaled, y_test_scaled).round(4)\n",
    "\n",
    "\n",
    "# saving AUC score\n",
    "knn_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = knn_pred).round(4)\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('knn Train-Test Gap :', abs(knn_train_score - knn_test_score).round(4))\n",
    "knn_test_gap = abs(knn_train_score - knn_test_score).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h3>c) Call the visual_cm function.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the visual_cm function\n",
    "visual_cm(true_y = y_test,\n",
    "          pred_y = knn_pred,\n",
    "          labels = ['Is Alive', 'Is Not Alive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h3>d) Confusion matrix.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "knn_tree_tn, \\\n",
    "knn_tree_fp, \\\n",
    "knn_tree_fn, \\\n",
    "knn_tree_tp = confusion_matrix(y_true = y_test, y_pred = knn_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {pruned_tree_tn}\n",
    "False Positives: {pruned_tree_fp}\n",
    "False Negatives: {pruned_tree_fn}\n",
    "True Positives : {pruned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VIII: Hyperparameter Tuning\n",
    "\n",
    "### a. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# importing packages\n",
    "########################################\n",
    "\n",
    "# new packages\n",
    "from sklearn.model_selection import RandomizedSearchCV   # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer                  # customizable scorer\n",
    "\n",
    "\n",
    "########################################\n",
    "# explanatory variable sets\n",
    "########################################\n",
    "candidate_dict = {\n",
    "\n",
    " # full model\n",
    " 'logit_full'   : ['book1_A_Game_Of_Thrones',\t'book2_A_Clash_Of_Kings',\t'book3_A_Storm_Of_Swords',\t\n",
    "                   'book4_A_Feast_For_Crows',\t'book5_A_Dance_with_Dragons',\t'log_numDeadRelations',\t\n",
    "                   'm_dateOfBirth',\t'm_mother',\t'm_father',\t'm_heir',\t'isMarried',\t'isNoble',\t'gg_Unknown',\n",
    "                   'gg_male',\t'age',\t'log_popularity',\t'c_Unknown',\t'c_Northmen',\t'c_Ironborn',\t\n",
    "                   'c_Wildlings',\t'c_Valyrian',\t'c_Braavos',\t'c_Dornish',\t'number_of_names',\t\n",
    "                   'interaction_book1_log_numDeadRel',\t'interaction_book2_log_numDeadRel',\t\n",
    "                   'interaction_book3_log_numDeadRel',\t'interaction_book4_log_numDeadRel',\t\n",
    "                   'interaction_book5_log_numDeadRel',\t'interaction_book1_popularity',\t\n",
    "                   'interaction_book2_popularity',\t'interaction_book3_popularity',\t'interaction_book4_popularity',\t\n",
    "                   'interaction_book5_popularity',\t'interaction_book1_popularity_ddrela',\t\n",
    "                   'interaction_book2_popularity_ddrela',\t'interaction_book3_popularity_ddrela',\t\n",
    "                   'interaction_book4_popularity_ddrela',\t'interaction_book5_popularity_ddrela',\t\n",
    "                   'interaction_book1_popularity_male',\t'interaction_book2_popularity_male',\t\n",
    "                   'interaction_book3_popularity_male',\t'interaction_book4_popularity_male',\t\n",
    "                   'interaction_book5_popularity_male',\t'interaction_male_log_numDeadRel',\n",
    "  ],\n",
    " \n",
    "\n",
    " # significant variables only (set 1)\n",
    " 'logit_sig'    : ['book1_A_Game_Of_Thrones' , 'book3_A_Storm_Of_Swords' , \n",
    "                    'log_numDeadRelations' , \n",
    "                    'log_popularity' , 'interaction_book3_log_numDeadRel' ,  'interaction_book3_popularity' , \n",
    "                   'interaction_book3_popularity_ddrela'],\n",
    "    \n",
    "    \n",
    " # significant variables only (set 2)\n",
    " 'logit_sig_2'  : ['book1_A_Game_Of_Thrones' , 'book4_A_Feast_For_Crows' , \n",
    "                    'log_numDeadRelations' , 'log_popularity' ,  'interaction_book4_log_numDeadRel' ],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "########################################\n",
    "# checking previous model performances\n",
    "########################################\n",
    "# comparing results\n",
    "print(f\"\"\"\n",
    "Model         AUC Score    Training Accuracy  Testing Accuracy TN, FP, FN, TP     Train-Test gap\n",
    "-----         ---------    --------------     ---------------  --------------     --------------\n",
    "Logistic      {logreg_auc_score}        {logreg_train_score}            {logreg_test_score}           {logreg_tn, logreg_fp, logreg_fn, logreg_tp}    {logreg_test_gap}\n",
    "Full Tree     {full_tree_auc_score}        {full_tree_train_score}            {full_tree_test_score}           {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}   {full_tree_test_gap}\n",
    "Pruned Tree   {pruned_tree_auc_score}        {pruned_tree_train_score}            {pruned_tree_test_score}           {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}    {pruned_tree_test_gap}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Split the dataset into training and testing sets using logit_sig_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the logit_sig variables\n",
    "GOT_data   =  GOT.loc[ : , candidate_dict['logit_sig_2']]\n",
    "GOT_target =  GOT.loc[ : , 'isAlive']\n",
    "\n",
    "\n",
    "# train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            GOT_data,\n",
    "            GOT_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.10,\n",
    "            stratify     = GOT_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Logistic Regression with Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a logistic regression model with default values\n",
    "lr_default = LogisticRegression(solver = 'lbfgs',\n",
    "                                C = 1.0,\n",
    "                                warm_start = False,\n",
    "                                random_state = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FITTING the training data\n",
    "lr_default_fit = lr_default.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "lr_default_pred = lr_default_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', lr_default_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', lr_default_fit.score(x_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# SCORING with AUC\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = lr_default_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = lr_default_fit.score(x_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = lr_default_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC score\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = lr_default_pred).round(4)\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Logreg Train-Test Gap :', abs(logreg_train_score - logreg_test_score).round(4))\n",
    "Logreg_test_gap = abs(logreg_train_score - logreg_test_score).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IX: Hyperparameter Tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# RandomizedSearchCV\n",
    "########################################\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "C_range          = np.arange(0.1, 5.0, 0.1)\n",
    "warm_start_range = [True, False]\n",
    "solver_range     = ['newton-cg', 'sag', 'lbfgs']\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'C'          : C_range,\n",
    "              'warm_start' : warm_start_range,\n",
    "              'solver'     : solver_range}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "lr_tuned = LogisticRegression(random_state = 219,\n",
    "                              max_iter     = 1000) # increased for convergence\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "lr_tuned_cv = RandomizedSearchCV(estimator           = lr_tuned,   # the model object\n",
    "                                 param_distributions = param_grid, # parameters to tune\n",
    "                                 cv                  = 3,          # how many folds in cross-validation\n",
    "                                 n_iter              = 250,        # number of combinations of hyperparameters to try\n",
    "                                 random_state        = 219,        # starting point for random sequence\n",
    "                                 scoring = make_scorer(\n",
    "                                           roc_auc_score,\n",
    "                                           needs_threshold = False)) # scoring criteria (AUC)\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "lr_tuned_cv.fit(GOT_data, GOT_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", lr_tuned_cv.best_params_)\n",
    "print(\"Tuned CV AUC      :\", lr_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. RandomizedSearch CV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the results of RandomizedSearch CV\n",
    "lr_tuned_cv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Estimator for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the best estimator for the model\n",
    "lr_tuned_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Hyperparameter with RandomizedSearch CV tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "lr_tuned = LogisticRegression(C            = 4.8,\n",
    "                              warm_start   = False,\n",
    "                              solver       = 'sag',\n",
    "                              max_iter     = 1000,\n",
    "                              random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the model to the full dataset\n",
    "lr_tuned.fit(GOT_data, GOT_target) # this is ok because already tuned\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "lr_tuned_pred = lr_tuned.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('LR Tuned Training ACCURACY:', lr_tuned.score(x_train, y_train).round(4))\n",
    "print('LR Tuned Testing  ACCURACY:', lr_tuned.score(x_test, y_test).round(4))\n",
    "print('LR Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = lr_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "lr_tuned_train_score = lr_tuned.score(x_train, y_train).round(4) # accuracy\n",
    "lr_tuned_test_score  = lr_tuned.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "lr_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = lr_tuned_pred).round(4) # auc\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Logreg Tuned Train-Test Gap :', abs(lr_tuned_train_score - lr_tuned_test_score).round(4))\n",
    "lr_tuned_test_gap = abs(lr_tuned_train_score - lr_tuned_test_score).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "lr_tuned_tn, \\\n",
    "lr_tuned_fp, \\\n",
    "lr_tuned_fn, \\\n",
    "lr_tuned_tp = confusion_matrix(y_true = y_test, y_pred = lr_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {lr_tuned_tn}\n",
    "False Positives: {lr_tuned_fp}\n",
    "False Negatives: {lr_tuned_fn}\n",
    "True Positives : {lr_tuned_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Comparing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "lr_train_acc = lr_tuned.score(x_train, y_train).round(4)\n",
    "lr_test_acc  = lr_tuned.score(x_test, y_test).round(4)\n",
    "lr_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = lr_tuned_pred).round(4)\n",
    "lr_auc_test_gap = abs(lr_train_acc -lr_test_acc).round(4)\n",
    "\n",
    "# appending to print list\n",
    "# comparing results\n",
    "print(f\"\"\"\n",
    "Model         AUC Score    Training Accuracy  Testing Accuracy TN, FP, FN, TP     Train-Test gap\n",
    "-----         ---------    --------------     ---------------  --------------     --------------\n",
    "Logistic      {logreg_auc_score}        {logreg_train_score}            {logreg_test_score}           {logreg_tn, logreg_fp, logreg_fn, logreg_tp}    {logreg_test_gap}\n",
    "Full Tree     {full_tree_auc_score}        {full_tree_train_score}            {full_tree_test_score}           {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}   {full_tree_test_gap}\n",
    "Pruned Tree   {pruned_tree_auc_score}        {pruned_tree_train_score}            {pruned_tree_test_score}           {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}    {pruned_tree_test_gap}\n",
    "Tuned LR      {lr_auc}        {lr_train_acc}            {lr_test_acc}           {lr_tuned_tn,lr_tuned_fp,lr_tuned_fn,lr_tuned_tp}    {lr_auc_test_gap}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part X: Hyperparameter Tuning on Classification Trees\n",
    "\n",
    "### a) Tune the hyperparameters for a classification tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # declaring a hyperparameter space\n",
    "# criterion_range = ['gini', 'entropy']\n",
    "# splitter_range  = ['best', 'random']\n",
    "# depth_range     = np.arange(1, 25, 1)\n",
    "# leaf_range      = np.arange(1, 100, 1)\n",
    "\n",
    "\n",
    "# # creating a hyperparameter grid\n",
    "# param_grid = {'criterion'        : criterion_range,\n",
    "#               'splitter'         : splitter_range,\n",
    "#               'max_depth'        : depth_range,\n",
    "#               'min_samples_leaf' : leaf_range}\n",
    "\n",
    "\n",
    "# # INSTANTIATING the model object without hyperparameters\n",
    "# tuned_tree = DecisionTreeClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# # RandomizedSearchCV object\n",
    "# tuned_tree_cv = RandomizedSearchCV(estimator             = tuned_tree,\n",
    "#                                    param_distributions   = param_grid,\n",
    "#                                    cv                    = 3,\n",
    "#                                    n_iter                = 1000,\n",
    "#                                    random_state          = 219,\n",
    "#                                    scoring = make_scorer(roc_auc_score,\n",
    "#                                              needs_threshold = False))\n",
    "\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# tuned_tree_cv.fit(GOT_data, GOT_target)\n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters  :\", tuned_tree_cv.best_params_)\n",
    "# print(\"Tuned Training AUC:\", tuned_tree_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Build a classification tree model based on the hyperparameter tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "tree_tuned = DecisionTreeClassifier(splitter         = 'random',\n",
    "                                    min_samples_leaf = 6,\n",
    "                                    max_depth        = 6,\n",
    "                                    criterion        = 'gini',\n",
    "                                    random_state     = 219)\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "tree_tuned_fit = tree_tuned.fit(GOT_data, GOT_target)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "tree_tuned_pred = tree_tuned.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', tree_tuned.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', tree_tuned.score(x_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = tree_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "tree_tuned_train_score = tree_tuned.score(x_train, y_train).round(4) # accuracy\n",
    "tree_tuned_test_score  = tree_tuned.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "tree_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                       y_score = tree_tuned_pred).round(4) # auc\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Tree Tuned Train-Test Gap :', abs(tree_tuned_train_score - tree_tuned_test_score).round(4))\n",
    "Tree_tuned_test_gap = abs(tree_tuned_train_score - tree_tuned_test_score).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "tuned_tree_tn, \\\n",
    "tuned_tree_fp, \\\n",
    "tuned_tree_fn, \\\n",
    "tuned_tree_tp = confusion_matrix(y_true = y_test, y_pred = tree_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {tuned_tree_tn}\n",
    "False Positives: {tuned_tree_fp}\n",
    "False Negatives: {tuned_tree_fn}\n",
    "True Positives : {tuned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "tree_train_acc = tree_tuned.score(x_train, y_train).round(4)\n",
    "tree_test_acc  = tree_tuned.score(x_test, y_test).round(4)\n",
    "tree_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = tree_tuned_pred).round(4)\n",
    "tuned_tree_test_gap = abs(tree_train_acc -tree_test_acc).round(4)\n",
    "\n",
    "\n",
    "# appending to print list\n",
    "# comparing results\n",
    "print(f\"\"\"\n",
    "Model         AUC Score    Training Accuracy  Testing Accuracy TN, FP, FN, TP     Train-Test gap\n",
    "-----         ---------    --------------     ---------------  --------------     --------------\n",
    "Logistic      {logreg_auc_score}        {logreg_train_score}            {logreg_test_score}           {logreg_tn, logreg_fp, logreg_fn, logreg_tp}    {logreg_test_gap}\n",
    "Full Tree     {full_tree_auc_score}        {full_tree_train_score}            {full_tree_test_score}           {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}   {full_tree_test_gap}\n",
    "Pruned Tree   {pruned_tree_auc_score}        {pruned_tree_train_score}            {pruned_tree_test_score}           {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}    {pruned_tree_test_gap}\n",
    "Tuned LR      {lr_auc}        {lr_train_acc}            {lr_test_acc}           {lr_tuned_tn,lr_tuned_fp,lr_tuned_fn,lr_tuned_tp}    {lr_auc_test_gap}\n",
    "Tuned Tree    {tree_auc}        {tree_train_acc}            {tree_test_acc}           {tuned_tree_tn,tuned_tree_fp,tuned_tree_fn,tuned_tree_tp}   {tuned_tree_test_gap}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Visual Tree output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setting figure size\n",
    "# plt.figure(figsize=(40, 10))\n",
    "\n",
    "\n",
    "# # developing a plotted tree\n",
    "# plot_tree(decision_tree = tree_tuned_fit, \n",
    "#           feature_names = GOT.columns,\n",
    "#           filled        = True, \n",
    "#           rounded       = True, \n",
    "#           fontsize      = 14)\n",
    "\n",
    "\n",
    "# # rendering the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part XII: Ensemble Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# importing packages\n",
    "########################################\n",
    "# essentials\n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import pandas            as pd  # data science essentials\n",
    "import numpy             as np  # mathematical essentials\n",
    "\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split   # train-test split\n",
    "from sklearn.metrics import roc_auc_score              # auc score\n",
    "from sklearn.model_selection import RandomizedSearchCV # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer                # customizable scorer\n",
    "from sklearn.metrics import confusion_matrix           # confusion matrix\n",
    "\n",
    "\n",
    "# new tools\n",
    "from sklearn.ensemble import RandomForestClassifier     # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gbm\n",
    "\n",
    "\n",
    "########################################\n",
    "# explanatory variable sets\n",
    "########################################\n",
    "candidate_dict = {\n",
    "\n",
    " # full model\n",
    " 'logit_full'   : ['book1_A_Game_Of_Thrones',\t'book2_A_Clash_Of_Kings',\t'book3_A_Storm_Of_Swords',\t\n",
    "                   'book4_A_Feast_For_Crows',\t'book5_A_Dance_with_Dragons',\t'log_numDeadRelations',\t\n",
    "                   'm_dateOfBirth',\t'm_mother',\t'm_father',\t'm_heir',\t'isMarried',\t'isNoble',\t'gg_Unknown',\n",
    "                   'gg_male',\t'age',\t'log_popularity',\t'c_Unknown',\t'c_Northmen',\t'c_Ironborn',\t\n",
    "                   'c_Wildlings',\t'c_Valyrian',\t'c_Braavos',\t'c_Dornish',\t'number_of_names',\t\n",
    "                   'interaction_book1_log_numDeadRel',\t'interaction_book2_log_numDeadRel',\t\n",
    "                   'interaction_book3_log_numDeadRel',\t'interaction_book4_log_numDeadRel',\t\n",
    "                   'interaction_book5_log_numDeadRel',\t'interaction_book1_popularity',\t\n",
    "                   'interaction_book2_popularity',\t'interaction_book3_popularity',\t'interaction_book4_popularity',\t\n",
    "                   'interaction_book5_popularity',\t'interaction_book1_popularity_ddrela',\t\n",
    "                   'interaction_book2_popularity_ddrela',\t'interaction_book3_popularity_ddrela',\t\n",
    "                   'interaction_book4_popularity_ddrela',\t'interaction_book5_popularity_ddrela',\t\n",
    "                   'interaction_book1_popularity_male',\t'interaction_book2_popularity_male',\t\n",
    "                   'interaction_book3_popularity_male',\t'interaction_book4_popularity_male',\t\n",
    "                   'interaction_book5_popularity_male',\t'interaction_male_log_numDeadRel'],\n",
    " \n",
    "\n",
    " # significant variables only (set 1)\n",
    " 'logit_sig'    : ['book1_A_Game_Of_Thrones' , 'book3_A_Storm_Of_Swords' , \n",
    "                    'log_numDeadRelations' , \n",
    "                    'log_popularity' , 'interaction_book3_log_numDeadRel' ,  'interaction_book3_popularity' , \n",
    "                   'interaction_book3_popularity_ddrela'],\n",
    "    \n",
    "    \n",
    " # significant variables only (set 2)\n",
    " 'logit_sig_2'  : ['book1_A_Game_Of_Thrones' , 'book4_A_Feast_For_Crows' , \n",
    "                    'log_numDeadRelations' , 'log_popularity' ,  'interaction_book4_log_numDeadRel' ]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "########################################\n",
    "# checking previous model performances\n",
    "########################################\n",
    "\n",
    "print(f\"\"\"\n",
    "Model         AUC Score    Training Accuracy  Testing Accuracy TN, FP, FN, TP     Train-Test gap\n",
    "-----         ---------    --------------     ---------------  --------------     --------------\n",
    "Logistic      {logreg_auc_score}        {logreg_train_score}            {logreg_test_score}           {logreg_tn, logreg_fp, logreg_fn, logreg_tp}    {logreg_test_gap}\n",
    "Full Tree     {full_tree_auc_score}        {full_tree_train_score}            {full_tree_test_score}           {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}   {full_tree_test_gap}\n",
    "Pruned Tree   {pruned_tree_auc_score}        {pruned_tree_train_score}            {pruned_tree_test_score}           {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}    {pruned_tree_test_gap}\n",
    "Tuned LR      {lr_auc}        {lr_train_acc}            {lr_test_acc}           {lr_tuned_tn,lr_tuned_fp,lr_tuned_fn,lr_tuned_tp}    {lr_auc_test_gap}\n",
    "Tuned Tree    {tree_auc}        {tree_train_acc}            {tree_test_acc}           {tuned_tree_tn,tuned_tree_fp,tuned_tree_fn,tuned_tree_tp}   {tuned_tree_test_gap}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('./analysis_images/Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split with the logit_sig variables\n",
    "GOT_data   =  GOT.loc[ : , candidate_dict['logit_sig_2']]\n",
    "GOT_target =  GOT.loc[ : , 'isAlive']\n",
    "\n",
    "\n",
    "# train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            GOT_data,\n",
    "            GOT_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.10,\n",
    "            stratify     = GOT_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part XIII: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a random forest model with default values\n",
    "rf_default = RandomForestClassifier(n_estimators     = 100,\n",
    "                                    criterion        = 'gini',\n",
    "                                    max_depth        = None,\n",
    "                                    min_samples_leaf = 1,\n",
    "                                    bootstrap        = True,\n",
    "                                    warm_start       = False,\n",
    "                                    random_state     = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FITTING the training data\n",
    "rf_default_fit = rf_default.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', rf_default_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', rf_default_fit.score(x_test, y_test).round(4))\n",
    "\n",
    "rf_train_score = rf_default_fit.score(x_train, y_train).round(4)\n",
    "rf_test_score = rf_default_fit.score(x_test, y_test).round(4)\n",
    "\n",
    "# saving AUC score\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = rf_default_fit_pred).round(4))\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Random Forest Train-Test Gap :', abs(rf_train_score - rf_test_score).round(4))\n",
    "Random_forest_test_gap = abs(rf_train_score - rf_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting feature importances\n",
    "plot_feature_importances(rf_default_fit, x_train, export = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "rf_tn, \\\n",
    "rf_fp, \\\n",
    "rf_fn, \\\n",
    "rf_tp = confusion_matrix(y_true = y_test, y_pred = rf_default_fit_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {rf_tn}\n",
    "False Positives: {rf_fp}\n",
    "False Negatives: {rf_fn}\n",
    "True Positives : {rf_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "rf_train_acc = rf_default_fit.score(x_train, y_train).round(4)\n",
    "rf_test_acc  = rf_default_fit.score(x_test, y_test).round(4)\n",
    "rf_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = rf_default_fit_pred).round(4)\n",
    "rf_test_gap = abs(rf_train_acc -rf_test_acc).round(4)\n",
    "\n",
    "\n",
    "# appending to print list\n",
    "# comparing results\n",
    "print(f\"\"\"\n",
    "Model                      AUC Score    Training Accuracy  Testing Accuracy TN, FP, FN, TP     Train-Test gap\n",
    "-----                      ---------    --------------     ---------------  --------------     --------------\n",
    "Logistic                  {logreg_auc_score}        {logreg_train_score}            {logreg_test_score}           {logreg_tn, logreg_fp, logreg_fn, logreg_tp}    {logreg_test_gap}\n",
    "Full Tree                 {full_tree_auc_score}        {full_tree_train_score}            {full_tree_test_score}           {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}   {full_tree_test_gap}\n",
    "Pruned Tree               {pruned_tree_auc_score}        {pruned_tree_train_score}            {pruned_tree_test_score}           {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}    {pruned_tree_test_gap}\n",
    "Tuned LR                  {lr_auc}        {lr_train_acc}            {lr_test_acc}           {lr_tuned_tn,lr_tuned_fp,lr_tuned_fn,lr_tuned_tp}    {lr_auc_test_gap}\n",
    "Tuned Tree                {tree_auc}        {tree_train_acc}            {tree_test_acc}           {tuned_tree_tn,tuned_tree_fp,tuned_tree_fn,tuned_tree_tp}   {tuned_tree_test_gap}\n",
    "Random Forest (Full)      {rf_auc}        {rf_train_acc}            {rf_test_acc}           {rf_tn,rf_fp,rf_fn,rf_tp}   {rf_test_gap}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FITTING the training data\n",
    "# rf_default_fit = rf_default.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# # PREDICTING based on the testing set\n",
    "# rf_default_fit_pred = rf_default_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# # declaring a hyperparameter space\n",
    "# estimator_range  = np.arange(100, 1100, 250)\n",
    "# leaf_range       = np.arange(1, 31, 10)\n",
    "# criterion_range  = ['gini', 'entropy']\n",
    "# bootstrap_range  = [True, False]\n",
    "# warm_start_range = [True, False]\n",
    "\n",
    "\n",
    "# # creating a hyperparameter grid\n",
    "# param_grid = {'n_estimators'     : estimator_range,\n",
    "#               'min_samples_leaf' : leaf_range,\n",
    "#               'criterion'        : criterion_range,\n",
    "#               'bootstrap'        : bootstrap_range,\n",
    "#               'warm_start'       : warm_start_range}\n",
    "\n",
    "\n",
    "# # INSTANTIATING the model object without hyperparameters\n",
    "# forest_grid = RandomForestClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# # GridSearchCV object\n",
    "# forest_cv = RandomizedSearchCV(estimator           = forest_grid,\n",
    "#                                param_distributions = param_grid,\n",
    "#                                cv         = 3,\n",
    "#                                n_iter     = 1000,\n",
    "#                                scoring    = make_scorer(roc_auc_score,\n",
    "#                                             needs_threshold = False))\n",
    "\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# forest_cv.fit(GOT_data, GOT_target)\n",
    "\n",
    "\n",
    "# # PREDICT step is not needed\n",
    "\n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters  :\", forest_cv.best_params_)\n",
    "# print(\"Tuned Training AUC:\", forest_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best estimators based on RandomizedSearchCV\n",
    "# forest_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving processing efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING with best_estimator\n",
    "forest_tuned = RandomForestClassifier(n_estimators   = 350,\n",
    "                                    min_samples_leaf = 1,\n",
    "                                    warm_start       = True,\n",
    "                                    criterion        = 'entropy',\n",
    "                                    random_state     = 219)\n",
    "\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "forest_tuned_fit = forest_tuned.fit(GOT_data, GOT_target)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "forest_tuned_pred = forest_tuned_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Forest Tuned Training ACCURACY:', forest_tuned.score(x_train, y_train).round(4))\n",
    "print('Forest Tuned Testing  ACCURACY:', forest_tuned.score(x_test, y_test).round(4))\n",
    "print('Forest Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                                       y_score = forest_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "forest_tuned_train_score = forest_tuned.score(x_train, y_train).round(4) # accuracy\n",
    "forest_tuned_test_score  = forest_tuned.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "forest_tuned_auc = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = forest_tuned_pred).round(4) # auc\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Random Forest Tuned Train-Test Gap :', abs(forest_tuned_train_score - forest_tuned_test_score).round(4))\n",
    "Random_forest_tuned_test_gap = abs(forest_tuned_train_score - forest_tuned_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting feature importances\n",
    "plot_feature_importances(forest_tuned_fit,\n",
    "                         train = x_train,\n",
    "                         export = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "tuned_rf_tn, \\\n",
    "tuned_rf_fp, \\\n",
    "tuned_rf_fn, \\\n",
    "tuned_rf_tp = confusion_matrix(y_true = y_test, y_pred = forest_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {tuned_rf_tn}\n",
    "False Positives: {tuned_rf_fp}\n",
    "False Negatives: {tuned_rf_fn}\n",
    "True Positives : {tuned_rf_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "tuned_rf_train_acc = forest_tuned_fit.score(x_train, y_train).round(4)\n",
    "tuned_rf_test_acc  = forest_tuned_fit.score(x_test, y_test).round(4)\n",
    "tuned_rf_auc       = roc_auc_score(y_true  = y_test,\n",
    "                                   y_score = forest_tuned_pred).round(4)\n",
    "tuned_rf_test_gap = abs(tuned_rf_train_acc -tuned_rf_test_acc).round(4)\n",
    "\n",
    "\n",
    "# appending to print list\n",
    "# comparing results\n",
    "print(f\"\"\"\n",
    "Model                        AUC Score    Training Accuracy  Testing Accuracy TN, FP, FN, TP     Train-Test gap\n",
    "-----                        ---------    --------------     ---------------  --------------     --------------\n",
    "Logistic                    {logreg_auc_score}        {logreg_train_score}            {logreg_test_score}           {logreg_tn, logreg_fp, logreg_fn, logreg_tp}    {logreg_test_gap}\n",
    "Full Tree                   {full_tree_auc_score}        {full_tree_train_score}            {full_tree_test_score}           {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}   {full_tree_test_gap}\n",
    "Pruned Tree                 {pruned_tree_auc_score}        {pruned_tree_train_score}            {pruned_tree_test_score}           {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}    {pruned_tree_test_gap}\n",
    "Tuned LR                    {lr_auc}        {lr_train_acc}            {lr_test_acc}           {lr_tuned_tn,lr_tuned_fp,lr_tuned_fn,lr_tuned_tp}    {lr_auc_test_gap}\n",
    "Tuned Tree                  {tree_auc}        {tree_train_acc}            {tree_test_acc}           {tuned_tree_tn,tuned_tree_fp,tuned_tree_fn,tuned_tree_tp}   {tuned_tree_test_gap}\n",
    "Random Forest (Full)        {rf_auc}        {rf_train_acc}            {rf_test_acc}           {rf_tn,rf_fp,rf_fn,rf_tp}   {rf_test_gap}\n",
    "Tuned Random Forest (Full)  {tuned_rf_auc}        {tuned_rf_train_acc}            {tuned_rf_test_acc}           {tuned_rf_tn,tuned_rf_fp,tuned_rf_fn,tuned_rf_tp}    {tuned_rf_test_gap}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part XIV: Gradient Boosted Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_default = GradientBoostingClassifier(loss          = 'deviance',\n",
    "                                              learning_rate = 0.1,\n",
    "                                              n_estimators  = 100,\n",
    "                                              criterion     = 'friedman_mse',\n",
    "                                              max_depth     = None,\n",
    "                                              warm_start    = False,\n",
    "                                              random_state  = 219)\n",
    "\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "full_gbm_default_fit = full_gbm_default.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "full_gbm_default_pred = full_gbm_default_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', full_gbm_default_fit.score(x_train, y_train).round(4))\n",
    "print('Testing ACCURACY :', full_gbm_default_fit.score(x_test, y_test).round(4))\n",
    "\n",
    "gbm_train_score = full_gbm_default_fit.score(x_train, y_train).round(4)\n",
    "gbm_test_score = full_gbm_default_fit.score(x_test, y_test).round(4)\n",
    "\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = full_gbm_default_pred).round(4))\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('GBM Train-Test Gap :', abs(gbm_train_score - gbm_test_score).round(4))\n",
    "gbm_test_gap = abs(gbm_train_score - gbm_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "gbm_default_tn, \\\n",
    "gbm_default_fp, \\\n",
    "gbm_default_fn, \\\n",
    "gbm_default_tp = confusion_matrix(y_true = y_test, y_pred = full_gbm_default_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {gbm_default_tn}\n",
    "False Positives: {gbm_default_fp}\n",
    "False Negatives: {gbm_default_fn}\n",
    "True Positives : {gbm_default_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCORING the model\n",
    "gbm_train_acc = full_gbm_default_fit.score(x_train, y_train).round(4)\n",
    "gbm_test_acc  = full_gbm_default_fit.score(x_test, y_test).round(4)\n",
    "gbm_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = forest_tuned_pred).round(4)\n",
    "gbm_test_gap = abs(gbm_train_acc -gbm_test_acc).round(4)\n",
    "\n",
    "\n",
    "\n",
    "# appending to print list\n",
    "# comparing results\n",
    "print(f\"\"\"\n",
    "Model                        AUC Score    Training Accuracy  Testing Accuracy TN, FP, FN, TP     Train-Test gap\n",
    "-----                        ---------    --------------     ---------------  --------------     --------------\n",
    "Logistic                    {logreg_auc_score}        {logreg_train_score}            {logreg_test_score}           {logreg_tn, logreg_fp, logreg_fn, logreg_tp}    {logreg_test_gap}\n",
    "Full Tree                   {full_tree_auc_score}        {full_tree_train_score}            {full_tree_test_score}           {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}   {full_tree_test_gap}\n",
    "Pruned Tree                 {pruned_tree_auc_score}        {pruned_tree_train_score}            {pruned_tree_test_score}           {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}    {pruned_tree_test_gap}\n",
    "Tuned LR                    {lr_auc}        {lr_train_acc}            {lr_test_acc}           {lr_tuned_tn,lr_tuned_fp,lr_tuned_fn,lr_tuned_tp}    {lr_auc_test_gap}\n",
    "Tuned Tree                  {tree_auc}        {tree_train_acc}            {tree_test_acc}           {tuned_tree_tn,tuned_tree_fp,tuned_tree_fn,tuned_tree_tp}   {tuned_tree_test_gap}\n",
    "Random Forest (Full)        {rf_auc}        {rf_train_acc}            {rf_test_acc}           {rf_tn,rf_fp,rf_fn,rf_tp}   {rf_test_gap}\n",
    "Tuned Random Forest (Full)  {tuned_rf_auc}        {tuned_rf_train_acc}            {tuned_rf_test_acc}           {tuned_rf_tn,tuned_rf_fp,tuned_rf_fn,tuned_rf_tp}    {tuned_rf_test_gap}\n",
    "GBM (Full)                  {gbm_auc}        {gbm_train_acc}            {gbm_test_acc}           {gbm_default_tn,gbm_default_fp,gbm_default_fn,gbm_default_tp}   {gbm_test_gap}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # declaring a hyperparameter space\n",
    "# learn_range        = np.arange(0.1, 2.2, 0.5)\n",
    "# estimator_range    = np.arange(100, 501, 25)\n",
    "# depth_range        = np.arange(2, 11, 2)\n",
    "# warm_start_range   = [True, False]\n",
    "\n",
    "# # creating a hyperparameter grid\n",
    "# param_grid = {'learning_rate' : learn_range,\n",
    "#               'max_depth'     : depth_range,\n",
    "#               'n_estimators'  : estimator_range,\n",
    "#               'warm_start'    : warm_start_range}\n",
    "\n",
    "\n",
    "# # INSTANTIATING the model object without hyperparameters\n",
    "# full_gbm_grid = GradientBoostingClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# # GridSearchCV object\n",
    "# full_gbm_cv = RandomizedSearchCV(estimator     = full_gbm_grid,\n",
    "#                            param_distributions = param_grid,\n",
    "#                            cv                  = 3,\n",
    "#                            n_iter              = 500,\n",
    "#                            random_state        = 219,\n",
    "#                            scoring             = make_scorer(roc_auc_score,\n",
    "#                                                  needs_threshold = False))\n",
    "\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# full_gbm_cv.fit(GOT_data, GOT_target)\n",
    "\n",
    "\n",
    "# # PREDICT step is not needed\n",
    "\n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters  :\", full_gbm_cv.best_params_)\n",
    "# print(\"Tuned Training AUC:\", full_gbm_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking the best estimator for the model\n",
    "# full_gbm_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING with best_estimator\n",
    "gbm_tuned = GradientBoostingClassifier(learning_rate = 0.1,\n",
    "                                       max_depth     = 2,\n",
    "                                       n_estimators  = 175,\n",
    "                                       warm_start    = True,\n",
    "                                       random_state  = 219)\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "gbm_tuned_fit = gbm_tuned.fit(GOT_data, GOT_target)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "gbm_tuned_pred = gbm_tuned_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', gbm_tuned_fit.score(x_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', gbm_tuned_fit.score(x_test, y_test).round(4))\n",
    "\n",
    "gbm_tuned_train_score = gbm_tuned_fit.score(x_train, y_train).round(4)\n",
    "gbm_tuned_test_score = gbm_tuned_fit.score(x_test, y_test).round(4)\n",
    "\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = gbm_tuned_pred).round(4))\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('GBM Tuned Train-Test Gap :', abs(gbm_tuned_train_score - gbm_tuned_test_score).round(4))\n",
    "gbm_tuned_test_gap = abs(gbm_tuned_train_score - gbm_tuned_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacking the confusion matrix\n",
    "gbm_tuned_tn, \\\n",
    "gbm_tuned_fp, \\\n",
    "gbm_tuned_fn, \\\n",
    "gbm_tuned_tp = confusion_matrix(y_true = y_test, y_pred = gbm_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {gbm_tuned_tn}\n",
    "False Positives: {gbm_tuned_fp}\n",
    "False Negatives: {gbm_tuned_fn}\n",
    "True Positives : {gbm_tuned_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "gbm_tuned_train_acc = gbm_tuned_fit.score(x_train, y_train).round(4)\n",
    "gbm_tuned_test_acc  = gbm_tuned_fit.score(x_test, y_test).round(4)\n",
    "gbm_tuned_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = gbm_tuned_pred).round(4)\n",
    "gbm_tuned_test_gap = abs(gbm_tuned_train_acc -gbm_tuned_test_acc).round(4)\n",
    "\n",
    "\n",
    "# appending to print list\n",
    "# comparing results\n",
    "print(f\"\"\"\n",
    "Model                        AUC Score    Training Accuracy  Testing Accuracy TN, FP, FN, TP     Train-Test gap\n",
    "-----                        ---------    --------------     ---------------  --------------     --------------\n",
    "Logistic                    {logreg_auc_score}        {logreg_train_score}            {logreg_test_score}           {logreg_tn, logreg_fp, logreg_fn, logreg_tp}    {logreg_test_gap}\n",
    "Full Tree                   {full_tree_auc_score}        {full_tree_train_score}            {full_tree_test_score}           {full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp}   {full_tree_test_gap}\n",
    "Pruned Tree                 {pruned_tree_auc_score}        {pruned_tree_train_score}            {pruned_tree_test_score}           {pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp}    {pruned_tree_test_gap}\n",
    "Tuned LR                    {lr_auc}        {lr_train_acc}            {lr_test_acc}           {lr_tuned_tn,lr_tuned_fp,lr_tuned_fn,lr_tuned_tp}    {lr_auc_test_gap}\n",
    "Tuned Tree                  {tree_auc}        {tree_train_acc}            {tree_test_acc}           {tuned_tree_tn,tuned_tree_fp,tuned_tree_fn,tuned_tree_tp}   {tuned_tree_test_gap}\n",
    "Random Forest (Full)        {rf_auc}        {rf_train_acc}            {rf_test_acc}           {rf_tn,rf_fp,rf_fn,rf_tp}   {rf_test_gap}\n",
    "Tuned Random Forest (Full)  {tuned_rf_auc}        {tuned_rf_train_acc}            {tuned_rf_test_acc}           {tuned_rf_tn,tuned_rf_fp,tuned_rf_fn,tuned_rf_tp}    {tuned_rf_test_gap}\n",
    "GBM (Full)(Final Model)     {gbm_auc}        {gbm_train_acc}            {gbm_test_acc}           {gbm_default_tn,gbm_default_fp,gbm_default_fn,gbm_default_tp}   {gbm_test_gap}\n",
    "Tuned GBM                   {gbm_tuned_auc}        {gbm_tuned_train_acc}            {gbm_tuned_test_acc}           {gbm_tuned_tn,gbm_tuned_fp,gbm_tuned_fn,gbm_tuned_tp}    {gbm_tuned_test_gap}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
